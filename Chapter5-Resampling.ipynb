{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 5: Resampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Two of the most important type of resampling is cross-validation and bootstrapping.\n",
    "    - Cross-validation can be used to estimate the test error associated with a given statistical learning method in order to evaluate its performance, or to select the appropriate level of flexibility. The process of evaluating a model’s performance is known as model assessment, whereas the process of selecting the proper level of flexibility for a model is known as model selection\n",
    "    -  The bootstrap is used in several contexts, most commonly to provide a measure of accuracy of a parameter estimate or of a given statistical learning method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Cross Validation\n",
    "- There will be a distinction in the error with the training and testing set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.1 The Validation Set Approach\n",
    "- Sometimes, you can have a holding set and training set which is split into two categories!\n",
    "- The bad thing about using two splits is that it can produce highly variable results!\n",
    "- In the validation approach, only a subset of the observations—those that are included in the training set rather than in the validation set—are used to fit the model. Since statistical methods tend to perform worse when trained on fewer observations, this suggests that the validation set error rate may tend to overestimate the test error rate for the model fit on the entire data set.\n",
    "- <img src=\"./images/IntroStats_Chp5_6.png\" width=700px>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.2 Leave-One-Out Cross-Validation\n",
    "- However, instead of creating two subsets of comparable size, a single observation (x1, y1) is used for the validation set, and the remaining observations {(x2, y2), . . . , (xn, yn)} make up the training set. The statistical learning method is fit on the n − 1 training observations, and a prediction ˆy1 is made for the excluded observation, using its value x1.\n",
    "- Meaning, that are training set is large and has more info. than splitting it into two sets. \n",
    "- In fact, we are splitting into the data into every single one of the datapoints except for one! This is insane, especially if the dataset is large!\n",
    "- The drawback is that when you test it, it can produce a high degree of variance since the data is smaller and can differ from one another\n",
    "- The LOOCV estimate for the test MSE is the average of these n test error estimates:\n",
    "- <img src=\"./images/IntroStats_Chp5.png\" width=250px>\n",
    "- Advanatages:\n",
    "    - Has far less bias\n",
    "    - LOOCV approach tends not to overestimate the test error rate as much as the validation set approach does.\n",
    "    - Second, in contrast to the validation approach which will yield different results when applied repeatedly due to randomness in the training/validation set splits, performing LOOCV multiple times will always yield the same results: there is no randomness in the training/validation set splits.\n",
    "- With least squares linear or polynomial regression, an amazing shortcut makes the cost of LOOCV the same as that of a single model fit! \n",
    "- Noticed that we are using the leverage statisics, to help access the amount that an observation influences its own fit.\n",
    "    - <img src=\"./images/IntroStats_Chp5_2.png\" width=250px>\n",
    "- H is the leverage statistics: In order to quantify an observation’s leverage, we compute the leverage statistic. A large value of this statistic indicates an observation with high leverage.\n",
    "    - <img src=\"./images/IntroStats_Chp5_3.png\" width=250px>\n",
    "    - <img src=\"./images/IntroStats_Chp5_4.png\" width=700px>\n",
    "\n",
    "- <img src=\"./images/IntroStats_Chp5_5.png\" width=700px>\n",
    "- There's much less randomness in this method because we are using almost of the data points except for one, meaning our result in the testing set or validation set will be similar since it will be trained by practically the same data set!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.3 k-Fold Cross-Validation\n",
    "- The k-fold is an alternative to the LOOCV model!\n",
    "- This approach involves randomly dividing the set of observations into k groups, or folds, of approximately equal size.\n",
    "- The first fold is treated as a validation set, and the method is fit on the remaining k − 1 folds.\n",
    "- The mean squared error, MSE, is then computed on the observations in the held-out fold.\n",
    "- <img src=\"./images/IntroStats_Chp5_7.png\" width=250px>\n",
    "- Unlike the LOOCV, we do not have to split the data into every single one of the datapoints except for one. We divide into k instead of n (the total # of observations)\n",
    "- Moreover, the CV is more compuational efficient that the LOOCV method\n",
    "- The variability might be a more than LOOCV, but the trade-off is worth it!\n",
    "- If we are performing this calculation to find the optimal (or the least MSE) this method shows that it quite effective (very similar to the LOOCV)\n",
    "- <img src=\"./images/IntroStats_Chp5_8.png\" width=750px>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.4 Bias-Variance Trade-Off for k-Fold Cross-Validation\n",
    "- The K-fold has a higher bias than the LOOCV bc we are using less data in our sample.\n",
    "- But the variance should also be taken into consideration\n",
    "    - It turns out that LOOCV has higher variance than does k-fold CV with k < n.\n",
    "    - Moreover, given that we focus on the result, the mean of one number is it by itself, one number which will vary! But using the result of many will overlap the results, hence the sample with K-fold will have lesspoints but closer to one another as oppose to LOOCV which will have more points that are separate! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.5 Cross-Validation on Classification Problems\n",
    "- This method can also be applied in the classification problems. Instead of using the MSE, we can focus on the number of instances it got wrong.\n",
    "- In the image below, we noticed that the Bayes performs better than the logistic regression (even when they use higher polynomial for the decision boundary). But we are aware of the performance, since we created the dataset and can compare our result with the actual. In real-life, we may not have the actual dataset. Hence, we can apply CV for these problems as well (k-fold, LOOCV)\n",
    "- <img src=\"./images/IntroStats_Chp5_10.png\" width=500px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Bootstrap\n",
    "- As a simple example, the bootstrap can be used to estimate the standard errors of the coefficients from a linear regression fit.\n",
    "- An example provided is the desire to lower risk. The following formula is an example of how risk could be minimized. Essentially, we would to minimize the variance associated with the values we are looking at.\n",
    "- σ2X = Var(X), σ2Y = Var(Y ), and σXY = Cov(X, Y ).\n",
    "- <img src=\"./images/IntroStats_Chp5_9.png\" width=300px>\n",
    "- The example pf stocks, (minimizing the variances) requires to know the actual dataset. Generally, we do not have info on the datset. Hence, we have to use historic data to derive estimation for our X and Y dataset.\n",
    "    - One of the way the author describes is stimulating data for X and Y 100 times, and then deriving alpha, or the 1000 times!\n",
    "- The author has the actual parameters of the dataset, σ2X = 1, σ2Y = 1.25, and σXY = 0.5 which calculates an alpha of 0.6.\n",
    "- Using the calculation described above, our alpha comes to 0.5996 with a std. deviation of 0.083.\n",
    "- But in practice, we cannot do this. So, we derived distinct datasets from the original dataset to emulate the desired procedure described above.\n",
    "- We randomly select n observations from the data set in order to produce a bootstrap data set, Z∗1. The sampling is performed with replacement, which means that the same observation can occur more than once in the bootstrap data set.\n",
    "- <img src=\"./images/IntroStats_Chp5_11.png\" width=500px>\n",
    "- This procedure is repeated B times for some large value of B, in order to produce B different bootstrap data sets, Z∗1, Z∗2, . . . , Z∗B, and B corresponding α estimates, ˆα∗1, ˆα∗2, . . . , ˆα∗B. The formula for the Bootstrap Method is: \n",
    "- <img src=\"./images/IntroStats_Chp5_12.png\" width=500px>\n",
    "- Th eimage represents extracting datasets from the population (left) and performing the bootstrap method (right). Notice how they are identical!\n",
    "- <img src=\"./images/IntroStats_Chp5_13.png\" width=500px>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
