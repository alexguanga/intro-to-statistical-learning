{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4: Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.1: The Logistic Model\n",
    "\n",
    "- In this chapter we discuss three of the most\n",
    "widely-used classifiers: logistic regression, linear discriminant analysis, and K-nearest neighbors.\n",
    "- To avoid this problem, we must model p(X) using a function that gives\n",
    "outputs between 0 and 1 for all values of X. Many functions meet this\n",
    "description. In logistic regression, we use the logistic function\n",
    "- Logistic Regression graph and mathematical concepts helps keep the number between 0 and 1\n",
    "- The quantity p(X)/[1−p(X)] is called the odds, and can take on any value odds\n",
    "between 0 and ∞.\n",
    "<img src=\"./images/IntroStats_Chp4.png\" width=250px>\n",
    "<img src=\"./images/IntroStats_Chp4_2.png\" width=250px>\n",
    "<img src=\"./images/IntroStats_Chp4_3.png\" width=250px>\n",
    "\n",
    "- Remember that a value change in the coefficient cannot give us an accurate description of the change in the y variable. But the sign does provide info on the X and y variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.2 Estimating the Regression Coefficients\n",
    "- This is not like the linear regression where it uses the least of sum of squares. Instead, it uses the Maximum Likelihod\n",
    "- ** The basic intuition behind using maximum likelihood to fit a logistic regression model is as follows: we seek estimates for β0 and β1 such that the predicted probability ˆp(xi) of default for each individual, corresponds as closely as possible to the individual’s observed default status. In other words, we try to find ˆ β0 and ˆ β1 such that plugging these estimates into the model for p(X), yields a number close to one for all individuals who defaulted, and a number close to zero for all individuals who did not. **\n",
    "- The coefficients β0 and β1 in are unknown, and must be estimated based on the available training data. \n",
    "- The basic intuition behind using maximum likelihood to fit a logistic regression model is as follows: \n",
    "    - we seek estimates for β0 and β1 such that the predicted probability ˆp(xi) of default for each individual, corresponds as closely as possible to the individual’s observed default status. \n",
    "- In other words, we try to find ˆ β0 and ˆ β1 such that plugging these estimates into the model for p(X) yields a number close to one for all individuals who defaulted, and a number close to zero for all individuals who did not.\n",
    "\n",
    "<img src=\"./images/intro-to-stats-chp4-2.png\" width=500px>\n",
    "\n",
    "- In the graph above, we can look at it like coefficient estimates and related information that result from fitting a logistic regression model on the Default data in order to predict the probability of default=Yes using balance\n",
    "- Understanding the coefficients:\n",
    "- A lot of the info, btw the linear and logisitic regression is similar. The logisitic regression uses z-stat, while a linear regression will use t-stat.\n",
    "    - If β1 is 0.0005, (β1 is balance), then a one-unit increase in balance will increase the log odds of default by 0.0005."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.4 Multiple Logistic Regression\n",
    "- Model 1:\n",
    "<img src=\"./images/2.png\" width=500px>\n",
    "- Model 2:\n",
    "<img src=\"./images/IntroStats_Chp4_4.png\" width=500px>\n",
    "- It's weird that student has  positive prob to default (meaning that students are more likely to default) against the second model where student are less likely to default.\n",
    "- Orange: Default rates for students, Blue: Default rates for non-students\n",
    "<img src=\"./images/IntroStats_Chp4_5.png\" width=700px>\n",
    "- Looking at the graph, we see that throughout the graph, student are less likely to default. (if balance is fixed). However, the average shows that non-students are less likely to default. This explains the difference btw both prob.\n",
    "- In other words, students are more likely to have large credit card balances, which tend to be associated with high default rates.\n",
    "- Thus, even though an individual student with a given credit card balance will tend to have a lower probability of default than a non-student with the same credit card balance, the fact that students on the whole tend to have higher credit card balances means that overall, students tend to default at a higher rate than non-students.\n",
    "- A student is riskier than a non-student if no information about the student’s credit card balance is available. However, that student is less risky than a non-student with the same credit card balance!\n",
    "- This illustrates the dangers and subtleties associated with performing regressions involving only a single predictor when other predictors may also be relevant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.5 Logistic Regression for >2 Response Classes\n",
    "\n",
    "- The two-class logistic\n",
    "regression models discussed in the previous sections have multiple-class\n",
    "extensions, but in practice they tend not to be used all that often. One of\n",
    "the reasons is that the method we discuss in the next section, discriminant analysis, is popular for multiple-class classification. So we do not go into\n",
    "the details of multiple-class logistic regression here, but simply note that\n",
    "such an approach is possible, and that software for it is available in R."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.4 Linear Discriminant Analysis\n",
    "\n",
    "- We now consider an alternative and less direct approach to estimating these probabilities. In this alternative approach, we model the distribution of the predictors X separately in each of the response classes (i.e. given Y ), and then use Bayes’ theorem to flip these around into estimates for Pr(Y = k|X = x). When these distributions are assumed to be normal, it turns out that the model is very similar in form to logistic regression.\n",
    "- **We will use Bayes Theorem.**\n",
    "\n",
    "\n",
    "- Why do we need another method, when we have logistic regression? There are several reasons:\n",
    "     - When the classes are well-separated, the parameter estimates for the logistic regression model are surprisingly unstable. Linear discriminant analysis does not suffer from this problem.\n",
    "     - If n is small and the distribution of the predictors X is approximately normal in each of the classes, the linear discriminant model is again more stable than the logistic regression model.\n",
    "     - Linear discriminant analysis is popular when we have more than two response classes.\n",
    "- Remember that k means the different possibility of Y. For example, in the famous spam case, k will be either 0 or 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4.1 Using Bayes’ Theorem for Classification\n",
    "- Let πk represent the overall or prior prior probability that a randomly chosen observation comes from the kth class; this is the probability that a given observation is associated with the kth category of the response variable Y\n",
    "- Let fk(X) ≡ Pr(X = x|Y = k) denote the density function of X for an observation that comes from the kth class.\n",
    "- In other words, fk(x) is relatively large if there is a high probability that an observation in the kth class has X ≈ x, and fk(x) is small if it is very unlikely that an observation in the kth class has X ≈ x. Then Bayes’ theorem states that\n",
    "- Much like Bayes, we are looking at the prior info. to make prediction\n",
    "- Let fk(X) ≡ Pr(X = x|Y = k)\n",
    "- In this expression, X is fixed, and x is allowed to vary over all real numbers.\n",
    "- In other words, we would word this as the prob that our observation x is X given that Y happens a certain amount of time.\n",
    "<img src=\"./images/IntroStats_Chp4_6.png\" width=300px>\n",
    "- We refer to pk(x) as the posterior probability that an observation posterior X = x belongs to the kth class. That is, it is the probability that the observation belongs to the kth class, given the predictor value for that observation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4.2 Linear Discriminant Analysis for p = 1\n",
    "- Let's assume we only have one predictor\n",
    "- Assumptions:\n",
    "    - fk(x) is normal or Gaussian\n",
    "        - <img src=\"./images/IntroStats_Chp4_7.png\" width=300px>\n",
    "    - let us further assume that σ2 1 = . . . = σ2K : that is, there is a shared variance term across all K classes, which for simplicity we can denote by σ2\n",
    "        - <img src=\"./images/IntroStats_Chp4_8.png\" width=300px>\n",
    "    - After taking the log of the function above, we get ...\n",
    "        - <img src=\"./images/IntroStats_Chp4_9.png\" width=300px>\n",
    "    - Bayes Decision Boundary\n",
    "        - <img src=\"./images/IntroStats_Chp4_10.png\" width=300px>\n",
    "- Linear Discriminant Analysis (assumptions):\n",
    "    - <img src=\"./images/IntroStats_Chp4_11.png\" width=300px> \n",
    "    - To reiterate, the LDA classifier results from assuming that the observations within each class come from a normal distribution with a class-specific mean vector and a common variance σ2, and plugging estimates for these parameters into the Bayes classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4.3 Linear Discriminant Analysis for p > 1\n",
    "- We now extend the LDA classifier to the case of multiple predictors. To do this, we will assume that X = (X1,X2, . . .,Xp) is drawn from a multivariate Gaussian (or multivariate normal) distribution, with a class-specific mean vector and a common covariance matrix.\n",
    "- To indicate that a p-dimensional random variable X has a multivariate Gaussian distribution, we write X ∼ N(μ,Σ). Here E(X) = μ is the mean of X (a vector with p components), and Cov(X) = Σ is the p × p covariance matrix of X.\n",
    "- <img src=\"./images/IntroStats_Chp4_12.png\" width=400px> \n",
    "- When using more than one predictor, the forumula changes. The forumula below, assigns the prob. to all the K's and is decided by where the formula below is the largest.\n",
    "- <img src=\"./images/IntroStats_Chp4_13.png\" width=400px> \n",
    "\n",
    "\n",
    "- The blue dashed line represents the fraction of defaulting customers that are incorrectly classified, and the orange dotted line indicates the fraction of errors among the non-defaulting customers.\n",
    "- <img src=\"./images/IntroStats_Chp4_14.png\" width=400px> \n",
    "\n",
    "\n",
    "- The name “ROC” is historic, and comes from communications theory. It is an acronym for receiver operating characteristics.\n",
    "- <img src=\"./images/IntroStats_Chp4_15.png\" width=400px> \n",
    "- <img src=\"./images/IntroStats_Chp4_16.png\" width=400px> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4.4 Quadratic Discriminant Analysis\n",
    "- However, unlike LDA, QDA assumes that each class has its own covariance matrix. That is, it assumes that an observation from the kth class is of the form X ∼ N(μk,Σk), where Σk is a covariance matrix for the kth class.\n",
    "- <img src=\"./images/IntroStats_Chp4_17.png\" width=500px> \n",
    "- Roughly speaking, LDA tends to be a better bet than QDA if there are relatively few training observations and so reducing variance is crucial. In contrast, QDA is recommended if the training set is very large, so that the variance of the classifier is not a major concern, or if the assumption of a common covariance matrix for the K classes is clearly untenable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 A Comparison of Classification Methods\n",
    "- The only difference between the two approaches lies in the fact that β0 and β1 are estimated using maximum likelihood, whereas c0 and c1 are computed using the estimated mean and variance from a normal distribution. This same connection btw LDA and logistic regression also holds for multidimensional data with p > 1.\n",
    "- LDA assumes that the observations are drawn from a Gaussian distribution with a common covariance matrix in each class, and so can provide some improvements over logistic regression when this assumption approximately holds. Conversely, logistic regression can outperform LDA if these Gaussian assumptions are not met.\n",
    "- KNN is a completely non-parametric approach: no assumptions are made about the shape of the decision boundary. Therefore, we can expect this approach to dominate LDA and logistic regression when the decision boundary is highly non-linear. On the other hand, KNN does not tell us which predictors are important; we don’t get a table of coefficients.\n",
    "- QDA serves as a compromise between the non-parametric KNN method and the linear LDA and logistic regression approaches. Since QDA assumes a quadratic decision boundary, it can accurately model a wider range of problems than can the linear methods. Though not as flexible as KNN, QDA can perform better in the presence of a limited number of training observations because it does make some assumptions about the form of the decision boundary.\n",
    "\n",
    "\n",
    "- TAKEAWAYS:\n",
    "    - When the true decision boundaries are linear, then the LDA and logistic regression approaches will tend to perform well.\n",
    "    - When the boundaries are moderately non-linear, QDA may give better results.\n",
    "    - Finally, for much more complicated decision boundaries, a non-parametric approach such as KNN can be superior."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
