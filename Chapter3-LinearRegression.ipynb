{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3: Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Simple Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2 Assessing the Accuracy of the Coefficient Estimates\n",
    "- The sample mean will never be the same as the population mean\n",
    "- But with enough datasets (a lot), the sample mean will a close approximation to the population mean\n",
    "- Standard Errors of the mean: the standard error tells us the average amount that this estimate ˆμ differs from the actual value of μ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.3 Assessing the Accuracy of the Model\n",
    "\n",
    "- **R2 Statistic**: The R2 statistic has an interpretational advantage over the RSE, since unlike the RSE, it always lies between 0 and 1. However, it can still be challenging to determine what is a good R2 value, and in general, this will depend on the application.\n",
    "- The function can sometimes as a blackbox where we are not too concerned with the formuala, rather that the function provides some level of confidence. \n",
    "- There are two types of error:\n",
    "     - reducible error: You might be able to find new models that can provide a better accuracy to the specific prediction. This is possible with tweaks.\n",
    "     - irreducible error: This can never be reduced. We will always have information that has some errors with our models. \n",
    "<img src=\"./images/Intro_Stats_1.png\" style=\"width: 400px;\"/>\n",
    "- Looking at the image, the left side is the improvements that model can achieve. We can find a betas to make the models more accurate! The left side, is the variance, and that's info. we cannot reduce. This can by the way we retrieve info. Things that we cannot control!\n",
    "- Two things to look when assesing the accuracy of the model: standard error (RSE) and the R-Squared statistic.\n",
    "- RSS: The error is represented by the difference the linear prediction and the actual observation.\n",
    "- What Is R-squared?: R-squared is a statistical measure of how close the data are to the fitted regression line. It is also known as the coefficient of determination, or the coefficient of multiple determination for multiple regression.\n",
    "- R-squared = Explained variation / Total variation, R-squared is always between 0 and 100%:\n",
    "- Thus if we see a graph where there is a lot variation, or the data is not within the model, then this would have a low r-squared. Likewise, if the observations resembles the model, then we would have a high r-squared!\n",
    "- RSE: The RSE is an estimate of the standard deviation of the error term. Roughly speaking, it is the average amount that the response will deviate from the true regression line. \n",
    "    - Example: In the case of the advertising data, we see from the linear regression output that the RSE is 3.26. In other words, actual sales in each market deviate from the true regression line by approximately 3,260 units, on average. Another way to think about this is that even if the model were correct and the true values of the unknown coefficients β0 and β1 were known exactly, any prediction of sales on the basis of TV advertising would still be off by about 3,260 units on average. Of course, whether or not 3,260 units is an acceptable prediction error depends on the problem context. \n",
    "    - In the advertising data set, the mean value of sales over all markets is approximately 14,000 units, and so the percentage error is 3,260/14,000 = 23%.\n",
    "    - So the premise is this, the threshold to be seperated from the correct responses. This number has to depend on the population we are looking, check its relativety! **A smaller number here is BETTER**\n",
    "- So to answer your second question, the difference between the RSE and the R2 is that the RSE tells you something about the inaccuracy of the model (in this case the regression line) given the observed data.\n",
    "- The R2 on the other hand tells you how much variation is explained by the model (i.e. the regression line) relative the variation that was explained by the mean alone (i.e. the simplest model).\n",
    "- Rememeber that we are using th eman bc it the simplest model we can use. When weapply a more complicated model, we have to fugure out its relative to the mean. Thus, we can use the RSe (but the RSe is the total), the model of sum squares performs this. The RSE is the overall change from the fitted to the exact!\n",
    "\n",
    "BIAS VS UNBIASED\n",
    "- Bias is the difference between the expected value of an estimator and the true value being estimated. For example the sample mean for a simple random sample (SRS) is an unbiased estimator of the population mean because if you take all the possible SRS's find their means, and take the mean of those means then you will get the population mean (for finite populations this is just algebra to show this). But if we use a sampling mechanism that is somehow related to the value then the mean can become biased, think of a random digit dialing sample asking a question about income. If there is positive correlation between number of phone numbers someone has and their income (poor people only have a few phone numbers that they can be reached at while richer people have more) then the sample will be more likely to include more people with higher incomes and therefore the mean income in the sample will tend to be higher than the population income.\n",
    "\n",
    "- https://medium.com/humansystemsdata/rse-vs-r%C2%B2-ba8fba098434\n",
    "- https://stats.stackexchange.com/questions/256726/linear-regression-what-does-the-f-statistic-r-squared-and-residual-standard-err"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Multiple Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2 Some Important Questions\n",
    "- When we perform multiple linear regression, we usually are interested in answering a few important questions.\n",
    "    - Is at least one of the predictors X1,X2, . . . , Xp useful in predictingthe response?\n",
    "    - Do all the predictors help to explain Y , or is only a subset of the predictors useful? \n",
    "    - How well does the model fit the data?\n",
    "    - Given a set of predictor values, what\n",
    "- Using F-Stats\n",
    "- <img src=\"./images/Intro_Stats_2.png\" style=\"width: 400px;\"/>\n",
    "- Hence, when there is no relationship between the response and predictors, one would expect the F-statistic to take on a value close to 1.\n",
    "- Given these individual p-values for each variable, why do we need to look at the overall F-statistic? After all, it seems likely that if any one of the p-values for the individual variables is very small, then at least one of the predictors is related to the response. However, this logic is flawed, especially when the number of predictors p is large.\n",
    "- However, the F-statistic does not suffer from this problem because it adjusts for the number of predictors. Hence, if H0 is true, there is only a 5% chance that the Fstatistic will result in a p-value below 0.05, regardless of the number of predictors or the number of observations.\n",
    "- As discussed in the previous section, the first step in a multiple regression analysis is to compute the F-statistic and to examine the associated pvalue.\n",
    "- Mallow’s Cp, Akaike information criterion (AIC), Bayesian information criterion (BIC), and adjusted R2 (Will be discussed later)\n",
    "- How to select which variables, too much features make it impossible to look at all the values\n",
    "    - **Forward Selection**: We begin with the null model—a model that contains an intercept but no predictors. We then fit p simple linear regressions and add to the null model the variable that results in the lowest RSS. We then add to that model the variable that results in the lower RSS for the new two-variable model.\n",
    "        - Essentially we are adding features until we find a variable that doesn't add any more information\n",
    "    - **Backward selection**: We start with all variables in the model, and remove the variable with the largest p-value—that is, the variable that is the least statistically significant. The new (p − 1)-variable model is fit, and the variable with the largest p-value is removed. This procedure continues until a stopping rule is reached. For instance, we may stop when all remaining variables have a p-value below some threshold. \n",
    "        - Here, we add all the values and keep removing until we find we are removing an important variable\n",
    "    - Mixed Selection: This is a combination of forward and backward selection. We start with no variables in the model, and as with forward selection, we add the variable that provides the best fit. We continue to add variables one-by-one. Of course, as we noted with the\n",
    "    - Advertising example, the p-values for variables can become larger as new predictors are added to the model. Hence, if at any point the p-value for one of the variables in the model rises above a certain threshold, then we remove that variable from the model. We continue to perform these forward and backward steps until all variables in the model have a sufficiently low p-value, and all variables outside the model would have a large p-value if added to the model.\n",
    "- Making Prediction is difficult because we have to first take into we are estimating the parameters, thus it will never be accurate. Second, we are predicting the sample from the popualtion, so the popualtion is prob. a bit different. Third, we know that there are errors just because of randomizes and we will never be able take to take into account. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Other Considerations in the Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.1 Qualitative Predictors\n",
    "- In our discussion so far, we have assumed that all variables in our linear regression model are quantitative. But in practice, this is not necessarily the case; often some predictors are qualitative.\n",
    "- <img src=\"./images/Mult_1.png\" style=\"width: 400px;\"/>\n",
    "- <img src=\"./images/Mult_2.png\" style=\"width: 400px;\"/>\n",
    "- <img src=\"./images/Mult_3.png\" style=\"width: 400px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.2 Extensions of the Linear Model \n",
    "- The additive assumption means that the effect of changes in a predictor Xj on the response Y is independent of the values of the other predictors. \n",
    "- The linear assumption states that the change in the response Y due to a one-unit change in Xj is constant, regardless of the value of Xjthat if we look at the variables in the linear equation, they are not quite independent. In fact, one variable might increase the slope of the other\n",
    "- In marketing, this is known as a synergy effect, and in statistics it is referred to as an interaction effect. This means that there's an interaction effect between the variables.\n",
    "- The hierarchical principle states that if we include an interaction in a model, we hierarchical should also include the main effects, even if the p-values associated with principle their coefficients are not significant. \n",
    "- In other words, if the interaction between X1 and X2 seems important, then we should include both X1 and X2 in the model even if their coefficient estimates have large p-values. \n",
    "- The rationale for this principle is that if X1 × X2 is related to the response, then whether or not the coefficients of X1 or X2 are exactly zero is of little interest.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.3 Potential Problems \n",
    "1. Non-linearity of the Data\n",
    "    - To check for non-linearity, we can plot the residuals against the predictor. Ideally, the residual plot will show no fitted discernible pattern. The presence of a pattern may indicate a problem with some aspect of the linear model.\n",
    "2. Correlation of Error Terms\n",
    "    - An important assumption of the linear regression model is that the error terms are uncorrelated. What does this mean? For instance, if the errors are uncorrelated, then the fact that error term is positive provides little or no information about the sign of any other error term.\n",
    "- Why might correlations among the error terms occur? Such correlations frequently occur in the context of time series data, which consists of observations for which measurements are obtained at discrete points in time.\n",
    "3. Non-constant Variance of Error Terms\n",
    "- Another important assumption of the linear regression model is that the error terms have a constant variance\n",
    "- magnitude of the residuals tends to increase with the fitted values.\n",
    "- An example is shown in the left-hand panel of Figure 3.11, sticity in which the magnitude of the residuals tends to increase with the fitted values.\n",
    "5. High Leverage Points\n",
    "- We just saw that outliers are observations for which the response yi is unusual given the predictor xi. In contrast, observations with high leverage have an unusual value for xi.\n",
    "6. Collinearity\n",
    "- Collinearity refers to the situation in which two or more predictor variables are closely related to one another.\n",
    "- A simple way to detect collinearity is to look at the correlation matrix of the predictors.\n",
    "- Multicollinearity - Unfortunately, not all collinearity problems can be detected by inspection of the correlation matrix: it is possible for collinearity to exist between three or more variables even if no pair of variables has a particularly high correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Comparison of Linear Regression with K-Nearest Neighbors \n",
    "- The thing about th elinear forumal is that it relies in a parametrics approach. \n",
    "- The answer is simple: the parametric approach will outperform the nonparametric approach if the parametric form that has been selected is close to the true form of f.\n",
    "- KNN performs slightly worse than linear regression when the relationship is linear, but much better than linear regression for non-linear situations.\n",
    "- But in higher dimensions, KNN often performs worse than linear regression."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
