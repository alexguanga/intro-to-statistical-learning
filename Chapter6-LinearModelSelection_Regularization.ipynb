{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 6: Linear ModelSelection & Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We will begin to see diferent changes (using other method other than the least of sum squares)\n",
    "- **Prediction Accuracy**: \n",
    "    - Provided that the true relationship between the response and the predictors is approximately linear, the least squares estimates will have low bias. If n >> p — that is, if n, the number of observations, is much larger than p, the number of variables—then the least squares estimates tend to also have low variance, and hence will perform well on test observations\n",
    "    - If n is not much larger than the p, then there can be a lot of variability in the least squares fit, resulting in overfitting and consequently poor predictions on future observations not used in model training\n",
    "    - If p is larger than then n, there is no longer a unique least squares coefficient estimate: the variance is infinite so the method cannot be used at all. By constraining or shrinking the estimated coefficients, we can often substantially reduce the variance at the cost of a negligible increase in bias\n",
    "- **Model Interpretability:**\n",
    "    - It is often the case that some or many of the variables used in a multiple regression model are in fact not associated with the response. Including such irrelevant variables leads to unnecessary complexity in the resulting model. By removing these variables—that is, by setting the corresponding coefficient estimates to zero—we can obtain a model that is more easily interpreted. Now least squares is extremely unlikely to yield any coefficient estimates that are exactly zero. In this chapter, we see some approaches for automatically performing feature selection or variable selection—that is, for excluding irrelevant variables from a multiple regression model.\n",
    "- Alterative Methods for using Least Sum of Squares:\n",
    "    - **Subset Selection**. This approach involves identifying a subset of the p predictors that we believe to be related to the response. We then fit a model using least squares on the reduced set of variables.\n",
    "    - **Shrinkage**. This approach involves fitting a model involving all p predictors. However, the estimated coefficients are shrunken towards zero relative to the least squares estimates. This shrinkage (also known as regularization) has the effect of reducing variance. Depending on what type of shrinkage is performed, some of the coefficients may be estimated to be exactly zero. Hence, shrinkage methods can also perform variable selection.\n",
    "    - **Dimension Reduction**. This approach involves projecting the p predictors into a M-dimensional subspace, where M < p. This is achieved by computing M different linear combinations, or projections, of the variables. Then these M projections are used as predictors to fit a linear regression model by least squares."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Subset Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.1 Best Subset Selection\n",
    "- We include all the possibility of p predictiors for all outcome (from 1 predictor, to two, and so on)\n",
    "- In order to select the best model, we must be able to use BIC, RSS, or Adjusted R-Squared.\n",
    "- The bad thing with this approach is that it would require a lot of computation, thus, hindering the speec and efficiency of the process.\n",
    "- Forward stepwise selection begins with a model containing no predictors, and then adds predictors to the model, one-at-a-time, until all of the predictors are in the model.\n",
    "- In particular, at each step the variable that gives the greatest additional improvement to the fit is added to the model.\n",
    "- <img src=\"./images/Reg_1.png\" width=500px>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.2 Stepwise Selection\n",
    "- **Forward Selection**\n",
    "    - It consider a smaller subsection that using all the models. \n",
    "    - In particular, at each step the variable that gives the greatest additional improvement to the fit is added to the model.\n",
    "    - One of the main issues with this process is  that a model might contain X1 but best two model is with X2 and X3. Hence, these best model of the two-variable will never come to reality!\n",
    "\n",
    "- <img src=\"./images/Reg_2.png\" width=500px>\n",
    "- <img src=\"./images/Reg_3.png\" width=500px>\n",
    "- **Backward Stepwise Selection**\n",
    "    - It begins with the full least squares model containing all p predictors, and then iteratively removes the least useful predictor, one-at-a-time.\n",
    "- <img src=\"./images/Reg_4.png\" width=500px>\n",
    "- **Hyprid Model**\n",
    "    - It uses the forward and backward selection. It's quite similat to the best subset model but its more computational efficient than the best subset model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.3 Choosing the Optimal Model\n",
    "- One of the pitfull using the RSS or R Sqaured is that they tend to overestimate the error function since it is evaulated on the training set error. There are two ways of powering through this, either using Cross Validation or add a bias (something extra) to the testing error! Using the R Squared or RSS, cannot be an accurate meaure since more data will only bring decrease the training error and not the testing error.\n",
    "\n",
    "- **Cp, AIC, BIC, and Adjusted R2**\n",
    "    - The training set MSE is generally an underestimate of the test MSE. (Recall that MSE = RSS/n.)\n",
    "    - Cp, AIC, BIC, and Adjusted R2 are fourth methods to help make the models more reliable!\n",
    "    - Cp Estimate:\n",
    "        - <img src=\"./images/Reg_5.png\" width=300px>\n",
    "        - ˆσ2 is an estimate of the variance of the error (e) associated with each response measurement\n",
    "        - d is the # of predictors\n",
    "        - Thus, as we increase the # of predictors, the value of Cp increases!\n",
    "        - We choose the model with the lowest Cp value\n",
    "        - In Figure 6.2, Cp selects the six-variable model containing the predictors income, limit, rating, cards, age and student.\n",
    "    - AIC Estimate:\n",
    "        - <img src=\"./images/Reg_6.png\" width=300px>\n",
    "        - Defined for a large class of models fit by the maximum likelihood\n",
    "        - Hence for least squares models, Cp and AIC are proportional to each other (reason it is omitted in the chart)\n",
    "    - BIC Estimate:\n",
    "        - <img src=\"./images/Reg_11.png\" width=300px> \n",
    "        - The BIC emerged from an Bayesian POV\n",
    "        - Notice that BIC replaces the 2dˆσ2 used by Cp with a log(n)dˆσ2 term, where n is the number of observations.\n",
    "        - Since logn > 2 for any n > 7, the BIC statistic generally places a heavier penalty on models with many variables, and hence results in the selection of smaller models than Cp.\n",
    "        - In the figure below, BIC chooses a model that contains only the four predictors income, limit, cards, and student\n",
    "    - Adjusted R Squared\n",
    "        - <img src=\"./images/Reg_9.png\" width=300px>\n",
    "        - Since RSS always decreases as more variables are added to the model, the R2 always increases as more variables are added.\n",
    "        - Thus, you must add a penalty or a larger constraint that the model must perform when it has more predictors\n",
    "        - Since adding noise variables leads to an increase in d, such variables will lead to an increase in RSS/(n−d−1), and consequently a decrease in the adjusted R2.\n",
    "            - Think of it. Comparing 3 predictors with 2 means, that the larger # of predictors decreases the denominator. Moreover, the increases in denominator means 1/3 or 1/2. The 1/2 is better. We should not include variables into the models unless we are sure that it will increase the adjusted R Square.\n",
    "        - Therefore, in theory, the model with the largest adjusted R2 will have only correct variables and no noise variables.\n",
    "        - Despite its popularity, and even though it is quite intuitive, the adjusted R2 is not as well motivated in statistical theory as AIC, BIC, and Cp.\n",
    "- <img src=\"./images/Reg_7.png\" width=500px> \n",
    "- **Validation and Cross-Validation**\n",
    "    - Performing validations can sometimes be better bc we are assuming less\n",
    "    - Also, we have the flexibility to use in a wider model selection task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Shrinkage Methods\n",
    "- As an alternative, we can fit a model containing all p predictors using a technique that constrains or regularizes the coefficient estimates, or equivalently, that shrinks the coefficient estimates towards zero\n",
    "- The two best-known techniques for shrinking the regression coefficients towards zero are ridge regression and the lasso."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.1 Ridge Regression\n",
    "- Ridge regression is very similar to least squares, except that the coefficients are estimated by minimizing a slightly different quantity\n",
    "- <img src=\"./images/Reg_8.png\" width=500px> \n",
    "- The lambda function is a shrinkage penalty.\n",
    "- It still a bit confusion how the ridge regression work. I understand the RSS tries to estimate the coefficients that maake the RSS small.\n",
    "- I get it! the reason that the coefficients become small is that when we increase the regularization term, we decrease the impact of the coefficients.\n",
    "-  It will decrease because the large lambda sort of acts like a help to the model. The model will not start from a blank place and instead it will start from a higher level of prediction indicating that the regularizaiton will not need to be as large as it would if there was no regularization term.\n",
    "- When we use he ridge ression over the least squares, we are attempting to variance but there will be slight increase in bias.\n",
    "- Overall, its a bit unclear but the underlying message is that we understand that the extra lambda will make the coefficient smaller since that we are pretyy much adding a bias to the formula (hence, the increase in bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.2 Lasso Regression\n",
    "- The misfortunate with the ridge regression is that the formula will always create and include all the preditors. \n",
    "- Even if we do not include all the predictor or use them, we will have to include them alll with a coefficient of 0\n",
    "- Instead of squaring the regularization term, it will include it in the absolute value\n",
    "- <img src=\"./images/Reg_10.png\" width=500px> \n",
    "- As with ridge regression, the lasso shrinks the coefficient estimates towards zero. However, in the case of the lasso, the L1 penalty has the effect of forcing some of the coefficient estimates to be exactly equal to zero when the tuning parameter λ is sufficiently large.\n",
    "- The lasso regression also serves as a variable selector\n",
    "- <img src=\"./images/Reg_12.png\" width=500px> \n",
    "- Hence, our equation must fit within the cosntraint. If we look at the bottom equation, we noticed that we are constraint by a circle (if we have 2 predictors)\n",
    "    - It makes more sense. We can now understand that the regularization term is simply a constraint that we impose in our equation. \n",
    "    - Moreover, s can be thought as a budget. When the budget is large, the RSS will not be as affected as if the s were lower (a more constraint budget)\n",
    "- **The Variable Selection Property of the Lasso**\n",
    "    - <img src=\"./images/Reg_13.png\" width=500px> \n",
    "    - Noticed that the shape of the ridge regression (circle) makes it impossible for the constraint to be tangent to the constraint!\n",
    "    - Since ridge regression has a circular constraint with no sharp points, this intersection will not generally occur on an axis, and so the ridge regression coefficient estimates will be exclusively non-zero.\n",
    "    - However, the lasso constraint has corners at each of the axes, and so the ellipse will often intersect the constraint region at an axis.\n",
    "- **Comparing the Lasso and Ridge Regression**\n",
    "    - Lasso can perform variable selection while rideg regression\n",
    "    - <img src=\"./images/Reg_14.png\" width=500px>\n",
    "    - Clearly the lasso leads to qualitatively similar behavior to ridge regression, in that as λ increases, the variance decreases and the bias increases.\n",
    "    - However, the variance of ridge regression is slightly lower than the variance of the lasso. Consequently, the minimum MSE of ridge regression is slightly smaller than that of the lasso.\n",
    "    - The lasso implicitly assumes that a number of the coefficients truly equal zero. Consequently, it is not surprising that ridge regression outperforms the lasso in terms of prediction error in this setting.\n",
    "    - One might expect the lasso to perform better in a setting where a relatively small number of predictors have substantial coefficients, and the remaining predictors have coefficients that are very small or that equal zero. \n",
    "    - Ridge regression will perform better when the response is a function of many predictors, all with coefficients of roughly equal size.\n",
    "    - The lasso regression can provide a smaller variance than the ridge regression. Variance means that our results are not going to differ from one another since we will have a smaller number of predictors\n",
    "- **Simple example of the lasso regression and the ridge regression**\n",
    "    - When we look a problem where we have no bias, we see that the ridge regression lower the coefficient of all the predictors by the same amount\n",
    "    - While the lasso regression has a threshold where we minimize it towards 0 if it is under some threshold\n",
    "- **Bayes and the lasso/ridge regression**\n",
    "    -  As we discussed, these regression rely on the probabilties that are coefficients have some kind of probility that they are what we entail them to be\n",
    "    - (β0, β1, . . . , βp)^Transpose\n",
    "    - The likelihood of the data can be written as f(Y |X, β), where X = (X1, . . . , Xp).\n",
    "    - <img src=\"./images/Reg_15.png\" width=300px>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.3 Selecting the Tuning Parameter\n",
    "- We choose a grid of λ values, and compute the cross-validation error for each value of λ, as described in Chapter 5. We then select the tuning parameter value for which the cross-validation error is smallest. Finally, the model is re-fit using all of the available observations and the selected value of the tuning parameter.\n",
    "- The tuning parameter is the lambda in the ridge or lasso regression\n",
    "- Overall, we are trying to reduce the error. Also, we also have the the predictors of the variables. We have to find the way to reduce the error whie maintaining the standarized coefficients to the lowest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 Dimension Reduction Methods\n",
    "- We will refer to these techniques as dimension reduction methods.\n",
    "- Hence, we will find method to better represent the variables\n",
    "- <img src=\"./images/Reg_16.png\" width=500px>\n",
    "- In other words, the dimension of the problem has been reduced from p + 1 to M + 1.\n",
    "- Pretty much, we are reducing the amount of variables to m \n",
    "- Using the same p predictors with the reduction ot m variables does not pose any added value to the reduction!\n",
    "- There are two method to decide the # of m variables, principal components and partial least squares."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3.1 Principal Components Regression\n",
    "- The first principal component direction of the data is that along which the observations vary the most.\n",
    "- When we create the variance, we look for the line that cannot have anymore variance\n",
    "- Given the image below, we can see that there will not be any line that can have a larger variance\n",
    "- Projecting a point onto a line simply involves finding the location on the line which is closest to the point.\n",
    "- <img src=\"./images/Reg_17.png\" width=500px>\n",
    "- Here φ11 = 0.839 and φ21 = 0.544 are the principal component loadings, which define the direction referred to above\n",
    "- In the image above, pop indicates the mean of all pop values in this data set, and ad indicates the mean of all advertising spending.\n",
    "- The idea is that out of every possible linear combination of pop and ad such that φ2 + φ2 = 1, this particular linear combination yields the highest variance\n",
    "- The first PCA tends to minimize the value from the line to the actual observations\n",
    "- The porjected observation onto the actual line provide the closest info (tends only work for the first PCA)\n",
    "- The line from left to right provides the score in the PCA that we've discusses. Lowest begins at 0, to the right where it increases\n",
    "- Hence, the first PCA provides info on the two variables, if the info is below the projected line than we can assume thta for both variables, the person has lower than average numbers\n",
    "- When most of the data is close to 0, then that meansthe projection will not be as useful as the previous PCA's\n",
    "- However, if we had other predictors, such as population age, income level, education, and so forth, then additional components could be constructed. They would successively maximize variance, subject to the constraint of being uncorrelated with the preceding components.\n",
    "- **The Principal Components Regression Approach**\n",
    "    - In other words, we assume that the directions in which X1, . . .,Xp show the most variation are the directions that are associated with Y\n",
    "    - This can prove to give better results isince we learn to avoid the noise\n",
    "    - PCR will tend to do well in cases when the first few principal components are sufficient to capture most of the variation in the predictors as well as the relationship with the response.\n",
    "    - In this sense, PCR is more closely related to ridge regression than to the lasso. In fact, one can show that PCR and ridge regression are very closely related. One can even think of ridge regression as a continuous version of PCR\n",
    "    - When performing PCR, we generally recommend standardizing each predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3.2 Partial Least Squares\n",
    "- PCR suffers from a drawback: there is no guarantee that the directions that best explain the predictors will also be the best directions to use for predicting the response.\n",
    "- We now present partial least squares (PLS), a supervised alternative to partial least PCR.\n",
    "- But unlike PCR, PLS identifies these new features in a supervised way—that is, it makes use of the response Y in order to identify new features that not only approximate the old features well, but also that are related to the response.\n",
    "- Thus, we figure out the variables that are most correlated to the y variable through the simple linear regression\n",
    "- When we perform the PCR, we use the values from the simple linear regression!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4 Considerations in High Dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4.1 High-Dimensional Data\n",
    "- Most problem are low-dimensional where we have a lot of observations but much fewer predictors\n",
    "- However, we data is new. We can get very specific amount of info on things, people, or places while the sample size might not be large. Thus, the structure of the data is flipped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4.2 What Goes Wrong in High Dimensions?\n",
    "- The reason is simple: regardless of whether or not there truly is a relationship between the features and the response, least squares will yield a set of coefficient estimates that result in a perfect fit to the data, such that the residuals are zero.\n",
    "    - Think about it. If we look at the data, and we have a lot of predictors, then our model will to overfit since we have a lot information that can make the accuracy high. However, this will not be indicative when we are using our models on the testing set\n",
    "- The regression can use a lot of information to create the model that it will overfit the data\n",
    "- The bad part is that the predictors do not have to have any predictive power for the models. The models will depict the noise rather than the signal when we are using large amount of data\n",
    "- For the training MSE, the more variables we have in the data will make the MSE decrease. However, for the testing MSE, the more variables makes the data worse!\n",
    "- Unfortunately, the Cp, AIC, and BIC approaches are not appropriate in the high-dimensional setting, because estimating ˆσ2 is problematic. (For instance, the formula for ˆσ2 from Chapter 3 yields an estimate ˆσ2 = 0 in this setting.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4.3 Regression in High Dimensions\n",
    "- Many of the methods seen in this chapter for fitting less flexible least squares models, such as forward stepwise selection, ridge regression, the lasso, and principal components regression, are particularly useful for performing regression in the high-dimensional setting.\n",
    "- Example: We can use 20, 50, or 100 predictors. There are 100 training examples\n",
    "    - <img src=\"./images/Reg_18.png\" width=500px>\n",
    "    - As the number of features increases, the test set error increases. When p = 20, the lowest validation set error was achieved when λ was small; however, when p was larger then the lowest validation set error was achieved using a larger value of λ.\n",
    "- Three important lessons: \n",
    "    - (1) regularization or shrinkage plays a key role in high-dimensional problems, \n",
    "    - (2) appropriate tuning parameter selection is crucial for good predictive performance, and\n",
    "    - (3) the test error tends to increase as the dimensionality of the problem (i.e. the number of features or predictors) increases, unless the additional features are truly associated with the response."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4.4 Interpreting Results in High Dimensions\n",
    "- In the high-dimensional setting, the multicollinearity problem is extreme: any variable in the model can be written as a linear combination of all of the other variables in the model. Essentially, this means that we can never know exactly which variables (if any) truly are predictive of the outcome, and we can never identify the best coefficients for use in the regression.\n",
    "- We must be careful not to overstate the results obtained, and to make it clear that what we have identified is simply one of many possible models for predicting blood pressure, and that it must be further validated on independent data sets.\n",
    "- You shoudl not use the pvalues, R squared, MSE or any other statistical metrics when analzying the training set as they can be miseading. Instead, we shoulf focus on the cross-validation error or on indepdent testing sets!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
