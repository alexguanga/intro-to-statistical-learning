{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 7: Moving Beyond Linearity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Polynomial regression extends the linear model by adding extra predictors, obtained by raising each of the original predictors to a power. For example, a cubic regression uses three variables, X, X2, and X3, as predictors. This approach provides a simple way to provide a nonlinear fit to data.\n",
    "- Step functions cut the range of a variable into K distinct regions in order to produce a qualitative variable. This has the effect of fitting a piecewise constant function.\n",
    "- Regression splines are more flexible than polynomials and step functions, and in fact are an extension of the two. They involve dividing the range of X into K distinct regions. Within each region, a polynomial function is fit to the data. However, these polynomials are constrained so that they join smoothly at the region boundaries, or knots. Provided that the interval is divided into enough regions, this can produce an extremely flexible fit.\n",
    "- Smoothing splines are similar to regression splines, but arise in a slightly different situation. Smoothing splines result from minimizing a residual sum of squares criterion subject to a smoothness penalty.\n",
    "- Local regression is similar to splines, but differs in an important way. The regions are allowed to overlap, and indeed they do so in a very smooth way.\n",
    "- Generalized additive models allow us to extend the methods above to deal with multiple predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 Polynomial Regression\n",
    "- Generally speaking, it is unusual to use d greater than 3 or 4 because for large values of d, the polynomial curve can become overly flexible and can take on some very strange shapes.\n",
    "- this is mainly used to understand the variance between the predictor and indepedent variables!\n",
    "- <img src=\"./images/Linearity_1.png\" width=500px>\n",
    "- We noticed in the chart that there are two of earners, one of them is above the 250 mark and others are below\n",
    "- The author uses a logisitic regression to find how age contributes to the person earning more than 250, the drawback is that people earning more than 250 is small relative to the entire population"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 Step Functions\n",
    "- Another way to do this is to create different \"subsets\"\n",
    "- This way, we can model each of the subset with their own specfic information\n",
    "- We pretty much break X into different bins\n",
    "- I(·) is an indicator function that returns a 1 if the condition is true, indicator and returns a 0 otherwise\n",
    "- The bad things about the step function is that the bins are difficult to adjust or name, it's very abritray\n",
    "- yi = β0 + β1C1(xi) + β2C2(xi) + . . . + βKCK(xi) + \u0003i.\n",
    "- The number that falls wihtin the bins is the average of the y values given the X's"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3 Basis Functions\n",
    "- We apply different functions to the values of X. For example, we might look at the the second power or the subgroup from the step functions\n",
    "- We can think of (7.7) as a standard linear model with predictors b1(xi), b2(xi), . . . , bK(xi).\n",
    "    - yi = β0 + β1b1(xi) + β2b2(xi) + β3b3(xi) + . . . + βKbK(xi) + error termi. (7.7)\n",
    "- The predictors can be thought of as the function\n",
    "- The idea is to have at hand a family of functions or transformations that can be applied to a variable X:\n",
    "- The basis function are chosen before in time (we know what are trying to maximize)\n",
    "- Regression splines are a type of basis function\n",
    "- For a poly function, a basis function can be a powered\n",
    "- Meaning, basis function are a function that can change or reshape the landscape of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.4 Regression Splines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4.1 Piecewise Polynomials\n",
    "- The coefficients β0, β1, β2, and β3 differ in different parts of the range of X. The points where the coefficients change are called knots.\n",
    "- We begin to implement the polynomal where it uses differnt parts of X\n",
    "- The vlaues of the coefficients change\n",
    "- For example, a piecewise cubic with no knots is just a standard cubic polynomial, as in  the image of Polynomial Regression with d = 3.\n",
    "- A piecewise cubic polynomial with a single knot at a point c takes the form\n",
    "- <img src=\"./images/Linearity_2.png\" width=500px>\n",
    "- Noticed that when we change the vlaues of the x, we look at differnt models. It'll be intersting to see the values that will be used to implement the values of X's\n",
    "- The first polynomial function has coefficients β01, β11, β21, β31, and the second has coefficients β02, β12, β22, β32.\n",
    "- Using more knots leads to a more flexible piecewise polynomial. In general, if we place K different knots throughout the range of X, then we will end up fitting K + 1 different cubic polynomials. Since we look at the equation one more than the number of knots\n",
    "- For example, we can instead fit piecewise linear functions. In fact, our piecewise constant functions of Section 7.2 are piecewise polynomials of degree 0!\n",
    "- <img src=\"./images/Linearity_3.png\" width=500px>\n",
    "- Noticed that in the image above, the left upper graph indicates that our function will be discontinues, meaning that there will be some opening in the graph. There's no way of holding the graph using the piecewise polynomials since we are using 4 differernt variables for each of the functions! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4.2 Constraints and Splines\n",
    "- The top right plot in Figure 7.3 shows the resulting fit.\n",
    "- As far as the constraint function that was used, its not explained by the author\n",
    "- In the lower left plot, we have added two additional constraints: now both the first and second derivatives of the piecewise polynomials are continuous at age=50.\n",
    "- We are requiring that the piecewise polynomial be not only continuous when age=50, but also very smooth.\n",
    "- So in the top left plot, we are using eight degrees of freedom, but in the bottom left plot we imposed three constraints (continuity, continuity of the first derivative, and continuity of the second derivative), meaning that it has 5 degree of freedom\n",
    "- In this case, we are looking at the degree of freedom as how free a variable could be. \n",
    "- Remember the reason it is 8, is that we have two equation with four variables!\n",
    "- Once we add some constraints, those variable are no longer free to be what they want to be!\n",
    "- When we impose new constraints, we are looking at the to decrease the degrees of freedoms\n",
    "- The curve in the bottom left plot is called a cubic spline.\n",
    "- The curve in the bottom left plot is called a cubic spline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4.3 The Spline Basis Representation\n",
    "- A cubic spline with K knots can be modeled as\n",
    "- <img src=\"./images/Linearity_4.png\" width=500px>\n",
    "- The model will be able to use least squares where we would minimize the total error of the functions\n",
    "- Remember that we can fit differnt polynomial functions to the data. For example, some polynomial equations will work for other part of the data (wages)\n",
    "- For the spline basis, we can also include a similar concept\n",
    "- The most direct way to represent a cubic spline using is to start off with a basis for a cubic polynomial—namely, x, x2, x3—and then add one truncated power basis function per knot\n",
    "- <img src=\"./images/Linearity_5.png\" width=500px>\n",
    "- The weird ξ is the truncated power basis function\n",
    "- One can show that adding a term of the form β4h(x, ξ) to the model (7.8) for a cubic polynomial will lead to a discontinuity in only the third derivative at ξ; the function will remain continuous, with continuous first and second derivatives, at each of the knots. \n",
    "    - So, the third derivative will not remain continous. We do not care about the third derivative. Thus, if the first and second deritivatives are continous, then our function will remain continous.\n",
    "- In other words, in order to fit a cubic spline to a data set with K knots, we perform least squares regression with an intercept and 3+K predictors, of the form X,X2,X3, h(X, ξ1), h(X, ξ2), . . . , h(X, ξK), where ξ1, . . . , ξK are the knots.\n",
    "- <img src=\"./images/Linearity_6.png\" width=500px>\n",
    "- I guess the reason we are using 4 is the three (poly to three) \n",
    "- A natural spline is a regression spline with additional boundary constraints: the function is required to be linear at the boundary (in the region where X is smaller than the smallest knot, or larger than the largest knot).\n",
    "- This additional constraint means that natural splines generally produce more stable estimates at the boundaries.\n",
    "- **A natural spline is a regression spline with additional boundary constraints**\n",
    "- **We see that the confidence bands in the boundary region appear fairly wild. A natural spline is a regression spline with additional boundary constraints: the function is required to be linear at the boundary (in the region where X is smaller than the smallest knot, or larger than the largest knot). This additional constraint means that natural splines generally produce more stable estimates at the boundaries.**\n",
    "- Inutuiton: If the data falls within the outer knots, we will see the confidence might appear a bit wild. We need to understand this so we would create a constraint where the function is required to be linear at the outer edge. This makes the problem more stable!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4.4 Choosing the Number and Locations of the Knots\n",
    "- One way to do this is to specify the desired degrees of freedom, and then have the software automatically place the corresponding number of knots at uniform quantiles of the data.\n",
    "- The reason it would have to be in three different percentile is because we have four degree of freedom?\n",
    "- One option is to try out different numbers of knots and see which produces the best looking curve\n",
    "- A somewhat more objective approach is to use cross-validation, as discussed in Chapters 5 and 6.\n",
    "- Natural spline can be thought as of naturally choosing boundaries that could fit with a certain guideline\n",
    "- The cubic spline is choosen to where YOU believe the data will be most apporpriate\n",
    "- <img src=\"./images/Linearity_7.png\" width=500px>\n",
    "- The reason that we are looking at the number of degree of freedom is bc we would like to know how our data should be set up\n",
    "- Four degree of freedom leads to three knots.\n",
    "- Of cource this can be somewhat technical and does not have a valid reasoning for it\n",
    "- In Section 7.7 we fit additive spline models simultaneously on several variables at a time. This could potentially require the selection of degrees of freedom for each variable. In cases like this we typically adopt a more pragmatic approach and set the degrees of freedom to a fixed number, say four, for all terms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4.5 Comparison to Polynomial Regression\n",
    "- Regression splines often give superior results to polynomial regression.\n",
    "- Regression splines often give superior results to polynomial regression. This is because unlike polynomials, which must use a high degree (exponent in the highest monomial term, e.g. X15) to produce flexible fits, splines introduce flexibility by increasing the number of knots but keeping the degree fixed.\n",
    "- Another reason that is helpful is that we do not have to keep a certain polynomail fixed regardless of the data. With splines, we have more flexibility to have the outcomes to be based on the data. \n",
    "- The extra flexibility in the polynomial produces undesirable results at the boundaries, while the natural cubic spline still provides a reasonable fit to the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.5 Smoothing Splines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.5.1 An Overview of Smoothing Splines\n",
    "- We create by specifying a set of knots, producing a sequence of basis functions, and then using least squares to estimate the spline coefficients.\n",
    "- **BASIS FUNCTION: The idea is to have at hand a family of functions or transformations that can be applied to a variable X: function b1(X), b2(X), . . . , bK(X).**\n",
    "- The one thing that is a bit unclear that in the function, we are estimating the basis function, and not the coefficient given to the basis function!\n",
    "- In fitting a smooth curve to a set of data, what we really want to do is find some function, say g(x), that fits the observed data well: that is, we want RSS = ∑ni = 1(yi − g(xi))2 to be small.\n",
    "- However, if we don’t put any constraints on g(xi), then we can always make RSS zero simply by choosing g such that it interpolates all of the yi. Such a function would woefully overfit the data—it would be far too flexible.\n",
    "- <img src=\"./images/Linearity_8.png\" width=500px>\n",
    "- λ is a nonnegative tuning parameter.\n",
    "- The function g that minimizes is known as a smoothing spline.\n",
    "- Equation 7.11 takes the “Loss+Penalty” formulation that we encounter in the context of ridge regression and the lasso in Chapter 6.\n",
    "- The term ∑ni = 1(yi − g(xi))2 is a loss function that encourages g to fit the data well, and the term λ g(t)''2dt is a penalty term\n",
    "- The notation with the double aestricks in the top is  the second derative of the g function\n",
    "- The first derivative g'(t) measures the slope of a function at t, and the second derivative corresponds to the amount by which the slope is changing\n",
    "- The second deritative tends to be the roughness of the line: Thinka bout it, as the first derative is near the min or max, the second deritivative will be 0\n",
    "    - If the second deritivative is 0\n",
    "    - As the line becomes more wiggly, we know that the deritiative gets larger in abs. value\n",
    "- (The second derivative of a straight line is zero; note that a line is perfectly smooth.)\n",
    "- The integral notation is an integral , which we can think of as a summation over the range of t. In other words, g''(t)2dt is simply a measure of the total change in the function g'(t), over its entire range\n",
    "- Intuition: If we look at the values of the integral portion, we figure that it's area under the curve. Thus, if the the first deritivative is constant, we know that the function will be a close to a straight line\n",
    "    - Thus, the integral of the second deritivative will be small\n",
    "    - Conversely, if g is jumpy and variable than g'(t) will vary significantly and the integral of g''(t)2dt will take on a large value.\n",
    "- Thus if this very small, it will not affect our function as much... However, if our line is not would have to pay a large penalty\n",
    "- The larger the value of λ, the smoother g will be\n",
    "    - Why? I am not sure why the an increase in lambda minimizes the penality?\n",
    "    - It increases the penality bc we want the value of the function to find a smooth function, thus, if we have a large lambda, we are able to find the smoothing of the function\n",
    "    - If not, when lambda is 0, we do not get the benefit of a smoothing function\n",
    "    \n",
    "- SMOOTHING LINES VS REGRESSION\n",
    "    - <img src=\"./images/Linearity_13.png\" width=400px> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.5.2 Choosing the Smoothing Parameter λ\n",
    "- We have seen that a smoothing spline is simply a natural cubic spline with knots at every unique value of xi.\n",
    "- It is possible to show that as λ increases from 0 to ∞, the effective degrees of freedom, which we write dfλ, decrease from n to 2.\n",
    "- **effective degrees of freedom instead of degrees of freedom**\n",
    "    - Usually degrees of freedom refer to the number of free parameters, such as the number of coefficients fit in a polynomial or cubic spline.\n",
    "    - Although a smoothing spline has n parameters and hence n nominal degrees of freedom, these n parameters are heavily constrained or shrunk down.\n",
    "    - Hence dfλ is a measure of the flexibility of the smoothing spline—the higher it is, the more flexible (and the lower-bias but higher-variance) the smoothing spline.\n",
    "        - When a function is more flexible, that we means we do not have a bias towards a specific strucutre. The lower bias with a higher variance means we might overfit the data through\n",
    "    - The definition of effective degrees of freedom is somewhat technical.\n",
    "- <img src=\"./images/Linearity_9.png\" width=200px>\n",
    "    - where ˆg is the solution to (7.11) for a particular choice of λ—that is, it is a n-vector containing the fitted values of the smoothing spline at the training points x1, . . . , xn.\n",
    "    - Equation 7.12 indicates that the vector of fitted values when applying a smoothing spline to the data can be written as a n × n matrix Sλ (for which there is a formula) times the response vector y.\n",
    "    - <img src=\"./images/Linearity_10.png\" width=200px> \n",
    "        - Which is the sum of the diagonal elements of the matrix Sλ.\n",
    "- In fitting a smoothing spline, we do not need to select the number or location of the knots—there will be a knot at each training observation, x1, . . . , xn. Instead, we have another problem: we need to choose the value of λ.\n",
    "- It turns out that the leaveone- out cross-validation error (LOOCV) can be computed very efficiently for smoothing splines, with essentially the same cost as computing a single fit\n",
    "- <img src=\"./images/Linearity_11.png\" width=400px> \n",
    "- The notation ˆg(−i)λ (xi) indicates the fitted value for this smoothing spline evaluated at xi, where the fit uses all of the training observations except for the ith observation (xi, yi).\n",
    "- In contrast, ˆgλ(xi) indicates the smoothing spline function fit to all of the training observations and evaluated at xi.\n",
    "- <img src=\"./images/Linearity_12.png\" width=400px> \n",
    "- Since there is little difference between the two fits, the smoothing spline fit with 6.8 degrees of freedom is preferable, since in general simpler models are better unless the data provides evidence in support of a more complex model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.6 Local Regression\n",
    "- Local regression is a different approach for fitting flexible non-linear functions, which involves computing the fit at a target point x0 using only the regression nearby training observations.\n",
    "- Intuition: We are looking at the observations that are around it!\n",
    "- Note that in Step 3 of Algorithm 7.1, the weights Ki0 will differ for each value of x0.\n",
    "- Pretty much, we will be using differernt observatiosn so we need to update it for the appropriate weight that we need to be using with the observations\n",
    "- <img src=\"./images/Linearity_15.png\" width=400px> \n",
    "- In order to perform local regression, there are a number of choices to be made, \n",
    "    - Such as how to define the weighting function K, \n",
    "    - Whether to fit a linear, constant, or quadratic regression in Step 3 above. \n",
    "    - The most important choice is the span s, defined in Step 1 above. The span plays a role like that of the tuning parameter λ in smoothing splines: it controls the flexibility of the non-linear fit.\n",
    "    - The smaller the value of s, the more local and wiggly will be our fit; alternatively, a very large value of s will lead to a global fit to the data using all of the training observations.\n",
    "- <img src=\"./images/Linearity_16.png\" width=400px> \n",
    "- span is a function of the nearest points we will be looking at divide the total space\n",
    "- if the span is larger, than we will see that the values are more genealized\n",
    "- The smaller the value of s, the more local and wiggly will be our fit; alternatively, a very large value of s will lead to a global fit to the data using all of the training observations.\n",
    "- the main point to use these weight is to filter that points that are not near the data point or mark that we are looking at "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.7 Generalized Additive Models\n",
    "- Here we explore the problem of flexibly predicting Y on the basis of several predictors, X1, . . . , Xp. This amounts to an extension of multiple linear regression.\n",
    "- Generalized additive models (GAMs) provide a general framework for extending a standard linear model by allowing non-linear functions of each of the variables, while maintaining additivity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.7.1 GAMs for Regression Problems\n",
    "- It is called an additive model because we calculate a separate fj for each Xj , and then add together all of their contributions.\n",
    "- We are creating a function for each of the variables and then adding them to porivide a finalzied answer\n",
    "- The beauty of GAMs is that we can use these methods as building blocks for fitting an additive model. In fact, for most of the methods that we have seen so far in this chapter, this can be done fairly trivially. Take, for example, natural splines, and consider the task of fitting the model\n",
    "- <img src=\"./images/Linearity_18.png\" width=400px> \n",
    "- Here year and age are quantitative variables, and education is a qualitative variable with five levels: HS, HS, More than Coll, Less than Coll, Coll, referring to the amount of high school or college education that an individual has completed. We fit the first two functions using natural splines. We fit the third function using a separate constant for each level, via the usual dummy variable approach of Section 3.3.1.\n",
    "    - Meaning that we look at the number that are qualitiative with some boundaries where the relationship changes\n",
    "    - The quantitative number (college years) is composed differently where we look at the info of the differernt categories differntly\n",
    "- <img src=\"./images/Linearity_17.png\" width=400px> \n",
    "- The left-hand panel indicates that holding age and education fixed, wage tends to increase slightly with year; this may be due to inflation.\n",
    "- The center panel indicates that holding education and year fixed, wage tends to be highest for intermediate values of age, and lowest for the very young and very old.\n",
    "- The right-hand panel indicates that holding year and age fixed, wage tends to increase with education: the more educated a person is, the higher their salary, on average. All of these findings are intuitive.\n",
    "- We do not have to use splines as the building blocks for GAMs: we can just as well use local regression, polynomial regression, or any combination of the approaches seen earlier in this chapter in order to create a GAM. GAMs are investigated in further detail in the lab at the end of this chapter.\n",
    "\n",
    "\n",
    "- Pros and Cons\n",
    "    - GAMs allow us to fit a non-linear fj to each Xj, so that we can automatically model non-linear relationships that std linear regression will miss. We do not need to manually try out many diff transformation on each vars.\n",
    "    - The non-linear fits can potentially make more accurate predictions for the response Y.\n",
    "    - Bc the model is additive, we can examine the effect of each Xj on Y individually while holding all of the other variables fixed.\n",
    "    - The smoothness of the function fj for the variable Xj can be summarized via degrees of freedom.\n",
    "    - The main limitation of GAMs is that the model is restricted to be additive. With many variables, important interactions can be missed. However, as with linear regression, we can manually add interaction terms to the GAM model by including additional predictors of the form Xj × Xk. In addition we can add low-dimensional interaction functions of the form fjk(Xj,Xk) into the model; such terms can be fit using two-dimensional smoothers such as local regression, or two-dimensional splines (not covered here).\n",
    "\n",
    "- GAMs provide a useful compromise between linear and fully nonparametric models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.7.2 GAMs for Classification Problems\n",
    "- Equation 7.18 is a logistic regression GAM. It has all the same pros and cons as discussed in the previous section for quantitative responses.\n",
    "- <img src=\"./images/Linearity_19.png\" width=400px> \n",
    "- <img src=\"./images/Linearity_20.png\" width=400px> \n",
    "- <img src=\"./images/Linearity_21.png\" width=400px> \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
