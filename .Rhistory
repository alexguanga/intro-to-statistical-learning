# 6.5.3 Choosing Among Models Using the Validation Set Approach and Cross-Validation
#=================================================================
set.seed(1)
training.set <- sample(c(TRUE, FALSE), nrow(Hitters), rep=TRUE)
testing.set <- (!training.set)
# Fitting the best subset model to our training set
regfit.best <- regsubsets(Salary~., data = Hitters[training.set, ], nvmax = 19)
# Computating the validation error on the testing set, using the best model for each model size
testing.set.matrix <- model.matrix(Salary~., data = Hitters[testing.set, ])
# Performing a for loop to get the best set for the variables
# Recall that %*% means matrix multiplication
# Hence, we are multiplying the matrix of values with the beta (best beta for each vals)
# error is using the RMSE
val.errors <- rep(NA, 19)
for (i in 1:19){
coeffi <- coef(regfit.best, id = i)
pred <- testing.set.matrix[, names(coeffi)]%*%coeffi
val.errors[i] <- mean((Hitters$Salary[testing.set]-pred)^2)
}
# Vector of the RMSE for the best models
val.errors
# Finding the variable that minimizes the RMSE
which.min(val.errors) # 10 variable model
coef(regfit.best, 10)
# There's no function for the regsubset
# From the textbook, this is the function to use for prediction
predict.regsubset <- function(object, newdata, id, ...) {
form <- as.formula(object$call[[2]])
mat <- model.matrix(form, newdata)
coefi <- coef(object, id=id)
xvars <- names(coefi)
mat[,xvars]%*%coefi
}
# Now, that we have a model, we should use the full dataset to get the coefficeints
# from the full dataset, which will have a larger dataste
# hence, a more accurate description
regfit.best <- regsubsets(Salary~., data = Hitters, nvmax = 19)
coef(regfit.best, 10) # coefficients slightly changed and even some variables changed
# Performing k-fold
k <- 10
set.seed(10)
folds <- sample(1:k, nrow(Hitters), replace = TRUE)
cv.errors <- matrix(NA, k, 19, dimnames = list(NULL, paste(1:19)))
# For loop through all the k-fold
# It its not reponsive to the kfold, then it will be used as the training set
# If it is, it will be the testing set
# For each split, we will performing the best variable method (from 1 through 19)
for(j in 1:k){
best.fit <- regsubsets(Salary~., data = Hitters[folds!=j, ], nvmax = 19)
for(i in 1:19) {
pred <- predict.regsubset(best.fit, Hitters[folds==j,], id = i)
cv.errors[j,i] <- mean((Hitters$Salary[folds == j]-pred)^2)
}
}
cv.errors
# This has given us a 10×19 matrix, of which the (i,j)th element corresponds
# to the test MSE for the ith cross-validation fold for the best j-variable model.
# USing the apply function to find the average
mean.cv.errors <- apply(cv.errors, 2, mean) # 2 indicated columns
par(mfrow = c(1,1))
plot(mean.cv.errors, type='b')
# best model is the 11-variable (index begins at 0 but is model 1)
reg.best <- regsubsets(Salary~., data = Hitters, nvmax = 19)
coef(reg.best, 11)
#=================================================================
# 6.6 Lab 2: Ridge Regression and the Lasso
#=================================================================
# Will be performing the ridge and lasso regression
# Preprocessing the data
# Model matrix is an efficent way to find the dummy variables and fix it, adn then
# use them in a matrix
X <- model.matrix(Salary~., Hitters)[,-1]
y <- Hitters$Salary
#=================================================================
# 6.6.1 Ridge Regression
#=================================================================
# alpha=0: ridge regression, alpha=1: lasso regression
library(glmnet)
# Remember, we need to pass values for the lambda parameter
grid <- 10^seq(10, -2, length = 100)
ridge.mod <- glmnet(X, y, alpha = 0, lambda = grid)
dim(coef(ridge.mod)) # There are 20 rows (one for each predictor) & 100 columns (one for each lambda
)
X <- model.matrix(Salary~., Hitters)[,-1]
y <- Hitters$Salary
# Alternative method to split the data
set.seed(1)
?sample
set.seed(1)
training.set <- sample(1:nrow(x), nrow(x)/2)
testing.set <- (-training.set)
set.seed(1)
training.set <- sample(1:nrow(X), nrow(X)/2)
testing.set <- (-training.set)
testing.set
y.test <- y[testing.set]
y.test
install.packages("glmnet")
library(glmnet)
# Remember, we need to pass values for the lambda parameter
grid <- 10^seq(10, -2, length = 100)
ridge.mod <- glmnet(X, y, alpha = 0, lambda = grid)
dim(coef(ridge.mod)) # There are 20 rows (one for each predictor) & 100 columns (one for each lambda)
ridge.mod
set.seed(1)
training.sample <- sample(1:nrow(X), nrow(X)/2)
testing.sample <- (-training.sample)
y.test <- y[testing.sample]
1e-12
ridge.mod <- glmnet(X[training.sample], y[training.sample],
alpha = 0, lambda = grid, thresh = 1e-12)
ridge.predict <- predict(ridge.mod, s = 4, newx = x[testing.sample, ])
mean((ridge.predict - y.test)^2)
ridge.mod <- glmnet(X[training.sample], y[training.sample],
alpha = 0, lambda = grid, thresh = 1e-12)
ridge.mod <- glmnet(X[training.sample,], y[training.sample],
alpha = 0, lambda = grid, thresh = 1e-12)
ridge.predict <- predict(ridge.mod, s = 4, newx = x[testing.sample, ])
ridge.predict <- predict(ridge.mod, s = 4, newx = X[testing.sample, ])
mean((ridge.predict - y.test)^2)
set.seed(1)
cv.out <- cv.glmnet(X[training.sample, ], y[training.sample], alpha = 0)
plot(cv.out)
bestlam <- cv.out$lambda.min
bestlam
ridge.predict <- predict(ridge.mod, s = bestlam, newx = X[testing.sample, ])
mean((ridge.predict - y.test)^2)
?glmnet
out <- glmnet(X, y, alpha = 0)
predict(out, type = 'coefficients', s = bestlam)[1:20, ]
predict(out, type = 'coefficients', s = bestlam)
predict(out, type = 'coefficients', s = bestlam)[1:20, ]
lasso.mod <- glmnet(X[training.sample,], y[training.sample],
alpha = 1, lambda = grid)
plot(lasso.mod)
set.seed(1)
cv.out <- cv.glmnet(X[training.sample, ], y[training.sample], alpha = 1)
plot(cv.out)
bestlam <- cv.out$lambda.min #211.7416
bestlam
ridge.predict <- predict(ridge.mod, s = bestlam, newx = X[testing.sample, ])
mean((ridge.predict - y.test)^2) #96015.51
set.seed(1)
cv.out <- cv.glmnet(X[training.sample, ], y[training.sample], alpha = 1)
plot(cv.out)
bestlam <- cv.out$lambda.min #16.780166
ridge.predict <- predict(ridge.mod, s = bestlam, newx = X[testing.sample, ])
mean((ridge.predict - y.test)^2) #97244.28
set.seed(1)
cv.out <- cv.glmnet(X[training.sample, ], y[training.sample], alpha = 1)
plot(cv.out)
bestlam <- cv.out$lambda.min #16.780166
ridge.predict <- predict(lasso.mod, s = bestlam, newx = X[testing.sample, ])
mean((ridge.predict - y.test)^2) #97244.28
# the lasso model with λ chosen by cross-validation contains only seven variables.
out <- glmnet(X, y, alpha = 1, lambda = grid)
lasso.coef <- predict(out, type = 'coefficients', s = bestlam)[1:20,]
out <- glmnet(X, y, alpha = 1, lambda = grid)
lasso.coef <- predict(out, type = 'coefficients', s = bestlam)[1:20,]
lasso.coef
lasso.coef[lasso.coef !=0]
install.packages("pls")
# 6.7.1 Principal Components Regression
library(pls)
# 6.7.1 Principal Components Regression
library(pls)
pcr.fit <- pcr(Salary~., data = Hitters, scale = TRUE, validation = 'CV')
summary(pcr.fit)
# K-Fold Cross Validation
# Function for recall, precision, f1.score, accuracy
measurePrecisionRecall <- function(actual_labels, predict){
confusion.matrix <- table(actual_labels, predict)
tn <- table(actual_labels, predict)[1]
fn <- table(actual_labels, predict)[2]
fp <- table(actual_labels, predict)[3]
tp <- table(actual_labels, predict)[4]
total <- tn+fn+fp+tp
accuracy <- (tn+tp)/total
precision <- tp/(tp+fp)
recall <- tp/(tp+fn)
f1.score <- 2*precision*recall/(precision+recall)
cat("Accuracy", accuracy, '\n')
cat("Precision", precision, '\n')
cat("Recall", recall, '\n')
cat("F1 Score", f1.score, '\n', '----', '\n')
return(c(accuracy, precision, recall, f1.score))
}
# Importing the dataset
df <- read.csv("./archive/Social_Network_Ads.csv")
df <- subset(df, select=c("Gender", "Age", "EstimatedSalary", "Purchased"))
# Changing the categorical data of gender
df$Gender <- factor(df$Gender,
levels = c('Male','Female'),
labels = c(0,1))
# Splitting the dataset into training and testing
library(caTools)
set.seed(123)
split <- sample.split(df$Purchased, SplitRatio = 0.8)
training.set <- subset(df, split == TRUE)
testing.set <- subset(df, split == FALSE)
# Checking the datatype of the variables
sapply(testing.set, class)
# Gender is a factor, must change it to integer
training.set$Gender <- as.numeric(training.set$Gender)
testing.set$Gender <- as.numeric(testing.set$Gender)
# Feature Scaling
testing.set[, c("Age","EstimatedSalary")] <-
scale(subset(testing.set, select = c("Age", "EstimatedSalary")))
training.set[, c("Age","EstimatedSalary")] <-
scale(subset(training.set, select = c("Age", "EstimatedSalary")))
####
# Fitting the SVM
####
library(e1071)
# For looping through the diffeernt type of possible kernels
# Checkin their performance
kernels <- c('polynomial', 'radial', 'sigmoid')
for (kernel in kernels){
classifier <- svm(Purchased ~.,
data = training.set,
type = 'C-classification',
kernel = kernel)
# Predicting the results
y.pred <- predict(classifier, newdata = testing.set[,c("Age", "EstimatedSalary", "Gender")])
# Make the confusion matrix
confusion.matrix <- table(testing.set$Purchased, y.pred)
print(kernel)
print(measurePrecisionRecall(testing.set$Purchased, y.pred))
}
# Applying the k-fold
# We are breaking the dataset into 10 pieces and then using one of those for the testing set
# the other 9 would be used as the training set
library(caret)
folds <- createFolds(training.set$Purchased, k = 10)
# For each of the folds, we are going to make a training fold and testing fold
# we will be the Gaussian kernel, and then creating our performance metrics
# thus, we will store the values of the accuracy into a vector
cv <- lapply(folds, function(x) {
training.fold <- training.set[-x,]
testing.fold <- training.set[x,]
classifier <- svm(Purchased ~.,
data = training.fold,
type = 'C-classification',
kernel = 'radial')
y.pred <- predict(classifier, newdata = testing.fold[,c("Age", "EstimatedSalary", "Gender")])
confusion.matrix <- table(testing.fold$Purchased, y.pred)
performance.metrics <- measurePrecisionRecall(y.pred, testing.fold$Purchased)
accuracy <- performance.metrics[1]
return(accuracy)
})
accuracy <- mean(as.numeric(cv)) #0.90625
# Changing the actors of our dependent variables to make it a classification model
training.set$Purchased <- as.factor(training.set$Purchased)
# Appyling Grid Search
# We are performing grid search to find the optimal parameter for the svmRadial!
classifier <- train(form = Purchased ~., data = training.set, method = 'svmRadial')
classifier # Optimal parameters: The final values used for the model were sigma = 0.5562473 and C = 1.
classifier$bestTune
# Reminder: Cohen's Kappa? Kappa = (Observed Accuracy - Expected Accuracy) / (1 - Expected Accuracy)
setwd("~/All_Projects/ds-education/TextbookCourses/TB-IntroStatLearning-James")
#=================================================================
# 6.5.1 Best Subset Selection
#=================================================================
# We will try to figure how to measure the salary of baseball players
library(ISLR)
fix(Hitters)
names(Hitters)
dim(Hitters) #322  20
sum(is.na(Hitters$Salary)) # there are 59 missing values
# Removing any null values
Hitters <- na.omit(Hitters)
dim(Hitters) #263  20
sum(is.na(Hitters$Salary)) # there are 0 missing values
# Using the leaps library to find the subset predictors (given)
library(leaps)
regfit.full <- regsubsets(Salary~., Hitters)
summary(regfit.full) # The top models preidctors are printed
# The top two-model uses Hits and CRBI
regfit.full <- regsubsets(Salary~., data = Hitters, nvmax = 19) # 19-variable model will be fitted
regfit.full.summary <- summary(regfit.full)
names(regfit.full.summary) # objects in the summary:
# "which"  "rsq"    "rss"    "adjr2"  "cp"     "bic"    "outmat" "obj"
regfit.full.summary$rsq # R squared increases as we have more values in the function
# Plotting the values (Adjusted R square, cp, etc)
par(mfrow=c(2,2))
plot(regfit.full.summary$rss, xlab = "Number of variables", ylab = "RSS", type = 'b')
# For the adjusted R square, we realize that it peaks!
plot(regfit.full.summary$adjr2, xlab = "Number of variables", ylab = "Adjusted R Square", type = 'b')
# The largest value for Adjusted R square
which.max(regfit.full.summary$adjr2) # 11 value (or index)
# plotting the point where the value is the largest
points(11, regfit.full.summary$adjr2[11], col = 'red3', cex = 2, pch = 20)
# Performing the same steps for C and BIC
# C, min value is 10
plot(regfit.full.summary$cp, xlab = "Number of variables", ylab = "Cp", type = 'b')
points(which.min(regfit.full.summary$cp),
regfit.full.summary$cp[which.min(regfit.full.summary$cp)], col = 'red3', cex = 2, pch = 20)
# BIC, min value is 6
plot(regfit.full.summary$bic, xlab = "Number of variables", ylab = "bic", type = 'b')
points(which.min(regfit.full.summary$bic),
regfit.full.summary$bic[which.min(regfit.full.summary$bic)], col = 'red3', cex = 2, pch = 20)
# Using the plot.regsubsets
# Error in plot.new() : figure margins too large
# Check what's the issue
# When we use the function, we need to look for the top row and look at the
# values that are associated (dark box)
par(mfrow=c(1,1))
plot(regfit.full, scale='Cp')
plot(regfit.full, scale='adjr2')
plot(regfit.full, scale='r2')
plot(regfit.full, scale='bic')
#=================================================================
# 6.5.2 Forward and Backward Stepwise Selection
#=================================================================
regfit.fwd <- regsubsets(Salary~., data = Hitters, nvmax = 19, method = 'forward')
summary(regfit.fwd)
regfit.bwd <- regsubsets(Salary~., data = Hitters, nvmax = 19, method = 'backward')
summary(regfit.bwd)
# The forward, backward, and best selector are identical!
# There's a difference in variables the forward, baclward, and best subset selector (7vars)
coef(regfit.full, 7) #   Hits        Walks       CAtBat        CHits       CHmRun    DivisionW      PutOuts
coef(regfit.fwd, 7) #  AtBat         Hits        Walks         CRBI       CWalks    DivisionW      PutOuts
coef(regfit.bwd, 7) #  AtBat         Hits        Walks        CRuns       CWalks    DivisionW      PutOuts
#=================================================================
# 6.5.3 Choosing Among Models Using the Validation Set Approach and Cross-Validation
#=================================================================
set.seed(1)
training.set <- sample(c(TRUE, FALSE), nrow(Hitters), rep=TRUE)
testing.set <- (!training.set)
# Fitting the best subset model to our training set
regfit.best <- regsubsets(Salary~., data = Hitters[training.set, ], nvmax = 19)
# Computating the validation error on the testing set, using the best model for each model size
testing.set.matrix <- model.matrix(Salary~., data = Hitters[testing.set, ])
# Performing a for loop to get the best set for the variables
# Recall that %*% means matrix multiplication
# Hence, we are multiplying the matrix of values with the beta (best beta for each vals)
# error is using the RMSE
val.errors <- rep(NA, 19)
for (i in 1:19){
coeffi <- coef(regfit.best, id = i)
pred <- testing.set.matrix[, names(coeffi)]%*%coeffi
val.errors[i] <- mean((Hitters$Salary[testing.set]-pred)^2)
}
# Vector of the RMSE for the best models
val.errors
# Finding the variable that minimizes the RMSE
which.min(val.errors) # 10 variable model
coef(regfit.best, 10)
# There's no function for the regsubset
# From the textbook, this is the function to use for prediction
predict.regsubset <- function(object, newdata, id, ...) {
form <- as.formula(object$call[[2]])
mat <- model.matrix(form, newdata)
coefi <- coef(object, id=id)
xvars <- names(coefi)
mat[,xvars]%*%coefi
}
# Now, that we have a model, we should use the full dataset to get the coefficeints
# from the full dataset, which will have a larger dataste
# hence, a more accurate description
regfit.best <- regsubsets(Salary~., data = Hitters, nvmax = 19)
coef(regfit.best, 10) # coefficients slightly changed and even some variables changed
# Performing k-fold
k <- 10
set.seed(10)
folds <- sample(1:k, nrow(Hitters), replace = TRUE)
cv.errors <- matrix(NA, k, 19, dimnames = list(NULL, paste(1:19)))
# For loop through all the k-fold
# It its not reponsive to the kfold, then it will be used as the training set
# If it is, it will be the testing set
# For each split, we will performing the best variable method (from 1 through 19)
for(j in 1:k){
best.fit <- regsubsets(Salary~., data = Hitters[folds!=j, ], nvmax = 19)
for(i in 1:19) {
pred <- predict.regsubset(best.fit, Hitters[folds==j,], id = i)
cv.errors[j,i] <- mean((Hitters$Salary[folds == j]-pred)^2)
}
}
cv.errors
# This has given us a 10×19 matrix, of which the (i,j)th element corresponds
# to the test MSE for the ith cross-validation fold for the best j-variable model.
# USing the apply function to find the average
mean.cv.errors <- apply(cv.errors, 2, mean) # 2 indicated columns
par(mfrow = c(1,1))
plot(mean.cv.errors, type='b')
# best model is the 11-variable (index begins at 0 but is model 1)
reg.best <- regsubsets(Salary~., data = Hitters, nvmax = 19)
coef(reg.best, 11)
#=================================================================
# 6.6 Lab 2: Ridge Regression and the Lasso
#=================================================================
# Will be performing the ridge and lasso regression
# Preprocessing the data
# Model matrix is an efficent way to find the dummy variables and fix it, and then
# use them in a matrix
X <- model.matrix(Salary~., Hitters)[,-1]
y <- Hitters$Salary
#=================================================================
# 6.6.1 Ridge Regression
#=================================================================
# alpha=0: ridge regression, alpha=1: lasso regression
library(glmnet)
# Remember, we need to pass values for the lambda parameter
grid <- 10^seq(10, -2, length = 100)
ridge.mod <- glmnet(X, y, alpha = 0, lambda = grid)
dim(coef(ridge.mod)) # There are 20 rows (one for each predictor) & 100 columns (one for each lambda)
# Values that have a larger lambda -> coefficients will be smaller
# Values that have a smaller lambda -> coefficients will be larger
# Alternative method to split the data
set.seed(1)
training.sample <- sample(1:nrow(X), nrow(X)/2)
testing.sample <- (-training.sample)
y.test <- y[testing.sample]
# lambda is 4?
ridge.mod <- glmnet(X[training.sample,], y[training.sample],
alpha = 0, lambda = grid, thresh = 1e-12)
ridge.predict <- predict(ridge.mod, s = 4, newx = X[testing.sample, ])
mean((ridge.predict - y.test)^2)
# using cross validation to find the number of lambda
set.seed(1)
cv.out <- cv.glmnet(X[training.sample, ], y[training.sample], alpha = 0)
plot(cv.out)
bestlam <- cv.out$lambda.min #211.7416
# What's the MSE associated with the lambda
ridge.predict <- predict(ridge.mod, s = bestlam, newx = X[testing.sample, ])
mean((ridge.predict - y.test)^2) #96015.51
out <- glmnet(X, y, alpha = 0)
predict(out, type = 'coefficients', s = bestlam)[1:20, ]
#=================================================================
# 6.6.2 The Lasso
#=================================================================
lasso.mod <- glmnet(X[training.sample,], y[training.sample],
alpha = 1, lambda = grid)
plot(lasso.mod)
set.seed(1)
cv.out <- cv.glmnet(X[training.sample, ], y[training.sample], alpha = 1)
plot(cv.out)
bestlam <- cv.out$lambda.min #16.780166
ridge.predict <- predict(lasso.mod, s = bestlam, newx = X[testing.sample, ])
mean((ridge.predict - y.test)^2) #100743.4
# the lasso model with λ chosen by cross-validation contains only seven variables.
out <- glmnet(X, y, alpha = 1, lambda = grid)
lasso.coef <- predict(out, type = 'coefficients', s = bestlam)[1:20,]
lasso.coef[lasso.coef != 0] # Only 7-variables are chosen for the model
#=================================================================
# 6.7 Lab 3: PCR and PLS Regression
#=================================================================
# 6.7.1 Principal Components Regression
library(pls)
set.seed(2)
# scale=TRUE has the effect of standardizing each predictor
pcr.fit <- pcr(Salary~., data = Hitters, scale = TRUE, validation = 'CV')
summary(pcr.fit)
?pcr
# Plotting the results
validationplot(pcr.fit, val.type='MSEP')
which.min(pcr.fit$ncomp)
which.min(pcr.fit$scores)
which.min(pcr.fit$model)
summary(pcr.fit)
pcr.fit$ncomp
pcr.fit$scores
pcr.fit
summary(pcr.fit$coefficients)
summary(pcr.fit$scores)
summary(pcr.fit$ncomp)
summary(pcr.fit$validation)
summary(pcr.fit$Xvar)
set.seed(1)
pcr.fit <- pcr(Salary~., data = Hitters, subset = training.set, scale = TRUE, validation = 'CV')
summary(pcr.fit)
validationplot(pcr.fit, val.type='MSEP')
validationplot(pcr.fit, val.type='MSEP')
summary(pcr.fit)
pcr.pred <- predict(pcr.fit, x[testing.set, ], ncomp = 7)
mean((pcr.pred - y.test)^2)
pcr.pred <- predict(pcr.fit, x[testing.set, ], ncomp = 7)
pcr.pred <- predict(pcr.fit, x[testing.sample, ], ncomp = 7)
pcr.pred <- predict(pcr.fit, X[testing.set, ], ncomp = 7)
mean((pcr.pred - y.test)^2)
pcr.pred <- predict(pcr.fit, X[testing.sample, ], ncomp = 7)
mean((pcr.pred - y.test)^2)
pcr.pred <- predict(pcr.fit, X[testing.sample, ], ncomp = 6)
mean((pcr.pred - y.test)^2)
pcr.pred <- predict(pcr.fit, X[testing.sample, ], ncomp = 7)
mean((pcr.pred - y.test)^2)
pcr.fit <- pcr(y~X, scale = TRUE, ncomp=6)
y
X
X[0]
X[1]
X[1,]
summary(pcr.fit)
pls.fit <- plsr(Salary~., data = Hitters, subset = training.set, scale = TRUE, validation = 'CV')
summary(pls.fit)
plsr.fit <- plsr(Salary~., data = Hitters, subset = training.set, scale = TRUE, validation = 'CV')
summary(plsr.fit)
plsr.fit <- plsr(Salary~., data = Hitters, subset = training.sample, scale = TRUE, validation = 'CV')
summary(plsr.fit)
training.sample
training.set
X[training.sample,]
training.sample
x[70,]
X[70,]
set.seed(1)
plsr.fit <- plsr(Salary~., data = Hitters, subset = training.sample, scale = TRUE, validation = 'CV')
summary(plsr.fit)
plsr.pred <- predict(plsr.fit, X[testing.sample, ], ncomp = 2)
mean((plsr.pred - y.test)^2)
plsr.fit <- plsr(y~X, scale = TRUE, ncomp=2)
summary(plsr.fit)
